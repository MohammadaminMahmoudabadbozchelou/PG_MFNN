{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.compat.v1'; 'tensorflow.compat' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff8b94d5b09d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScipyOptimizerInterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.compat.v1'; 'tensorflow.compat' is not a package"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='-1'\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.contrib.opt import ScipyOptimizerInterface\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "import random\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def hyper_initial(self, layers):\n",
    "        L = len(layers)\n",
    "        W = []\n",
    "        b = []\n",
    "        for l in range(1, L):\n",
    "            in_dim = layers[l-1]\n",
    "            out_dim = layers[l]\n",
    "            std = np.sqrt(2/(in_dim + out_dim))\n",
    "            weight = tf.Variable(tf.random_normal(shape=[in_dim, out_dim], stddev=std))\n",
    "            bias = tf.Variable(tf.zeros(shape=[1, out_dim]))\n",
    "            W.append(weight)\n",
    "            b.append(bias)\n",
    "\n",
    "        return W, b\n",
    "\n",
    "    def fnn(self, W, b, X, Xmin, Xmax):\n",
    "        A = 2.0*(X - Xmin)/(Xmax - Xmin) - 1.0\n",
    "        L = len(W)\n",
    "        for i in range(L-1):\n",
    "            A = tf.tanh(tf.add(tf.matmul(A, W[i]), b[i]))\n",
    "        Y = tf.add(tf.matmul(A, W[-1]), b[-1])\n",
    "        \n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "LOSS = []\n",
    "LOSS_LF = []\n",
    "LOSS_HF = []\n",
    "def callback(loss_,loss_lf_,loss_hf_):\n",
    "    global n\n",
    "    global LOSS\n",
    "    global LOSS_LF\n",
    "    global LOSS_HF\n",
    "    LOSS = np.hstack((LOSS,loss_))\n",
    "    LOSS_LF = np.hstack((LOSS_LF,loss_lf_))\n",
    "    LOSS_HF = np.hstack((LOSS_HF,loss_hf_))\n",
    "    n += 1\n",
    "    if n%1000 == 0:\n",
    "        print('n: %d, loss: %.3e, loss_lf: %.3e, loss_hf: %.3e'%(n, loss_, loss_lf_, loss_hf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YH1_train = np.genfromtxt('Vis_Train_H.txt')\n",
    "XH1_train = np.genfromtxt('ExT_Train_H.txt')\n",
    "XH2_train = np.genfromtxt('Shr_Train_H.txt')\n",
    "\n",
    "YH1_train = YH1_train.reshape((-1,1))\n",
    "XH1_train = XH1_train.reshape((-1,1))\n",
    "XH2_train = XH2_train.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YH1_test = np.genfromtxt('Vis_Test_H.txt')\n",
    "XH1_test = np.genfromtxt('ExT_Test_H.txt')\n",
    "XH2_test = np.genfromtxt('Shr_Test_H.txt')\n",
    "\n",
    "YH1_test = YH1_test.reshape((-1,1))\n",
    "XH1_test = XH1_test.reshape((-1,1))\n",
    "XH2_test = XH2_test.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YL1_train = np.genfromtxt('Vis_L.txt')\n",
    "XL1_train = np.genfromtxt('ExT_L.txt')\n",
    "XL2_train = np.genfromtxt('Shr_L.txt')\n",
    "\n",
    "y_lf = ((np.random.normal(1,0.025,len(YL1_train))) * YL1_train).reshape(1,-1).T\n",
    "x_lf = ((np.random.normal(1,0.025,len(XL1_train))) * XL1_train).reshape(1,-1).T\n",
    "t_lf = ((np.random.normal(1,0.025,len(XL2_train))) * XL2_train).reshape(1,-1).T\n",
    "\n",
    "X_lf = np.hstack((x_lf,t_lf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hf = YH1_train.reshape(1,-1).T\n",
    "x_hf = XH1_train.reshape(1,-1).T\n",
    "t_hf = XH2_train.reshape(1,-1).T\n",
    "\n",
    "X_hf = np.hstack((x_hf,t_hf))\n",
    "\n",
    "Xmin = X_hf.min(0)\n",
    "Xmax = X_hf.max(0)\n",
    "Ymin = y_hf.min(0)\n",
    "Ymax = y_hf.max(0)\n",
    "\n",
    "Xhmin = np.hstack((Xmin, Ymin))\n",
    "Xhmax = np.hstack((Xmax, Ymax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "layers_lf = [D] + 2*[20] + [1]\n",
    "layers_hf_nl = [D+1] + 2*[10] + [1]\n",
    "layers_hf_l = [D+1] + [1]\n",
    "\n",
    "x_train_lf = tf.placeholder(shape=[None, D], dtype=tf.float32)\n",
    "y_train_lf = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "x_train_hf = tf.placeholder(shape=[None, D], dtype=tf.float32)\n",
    "y_train_hf = tf.placeholder(shape=[None, 1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN()\n",
    "W_lf, b_lf = model.hyper_initial(layers_lf)\n",
    "W_hf_nl, b_hf_nl = model.hyper_initial(layers_hf_nl)\n",
    "W_hf_l, b_hf_l = model.hyper_initial(layers_hf_l)\n",
    "\n",
    "y_pred_lf = model.fnn(W_lf, b_lf, x_train_lf, Xmin, Xmax)\n",
    "#low-fidelity values to serve as the input for the high-fidelity NN\n",
    "y_pred_lf_hf = model.fnn(W_lf, b_lf, x_train_hf, Xmin, Xmax)\n",
    "#nonlinear part with two inputs (x_H, y_l(x_H))\n",
    "y_pred_hf_nl = model.fnn(W_hf_nl, b_hf_nl, tf.concat([x_train_hf, y_pred_lf_hf], 1), Xhmin, Xhmax)\n",
    "#linear part with two inputs (x_H, y_l(x_H))\n",
    "y_pred_hf_l = model.fnn(W_hf_l, b_hf_l, tf.concat([x_train_hf, y_pred_lf_hf], 1), Xhmin, Xhmax)\n",
    "y_pred_hf = y_pred_hf_l + y_pred_hf_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_l2 = tf.add_n([tf.nn.l2_loss(w_) for w_ in W_hf_nl])\n",
    "loss_lf = tf.reduce_mean((tf.square(y_pred_lf - y_train_lf)))\n",
    "loss_hf =  tf.reduce_mean((tf.square(y_pred_hf - y_train_hf)))\n",
    "loss = loss_lf + loss_hf + loss_l2\n",
    "train_adam = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "train_lbfgs = ScipyOptimizerInterface(loss,\n",
    "                                      method = 'L-BFGS-B', \n",
    "                                      options={'maxiter': 50000,\n",
    "                                               'ftol': 1.0*np.finfo(float).eps\n",
    "                                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the cell below just only for model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = saver.restore(sess, 'MPINNModel/MPINN_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nmax1 = 2000\n",
    "#nmax2 = 20000\n",
    "#loss_c = 1.0e-6\n",
    "loss_ = 1.0\n",
    "train_dict = {x_train_lf: X_lf, y_train_lf: y_lf, x_train_hf: X_hf, y_train_hf: y_hf}\n",
    "print('Adam Optimizer')\n",
    "while n < 20000 and loss_ > 1e-6:\n",
    "    n += 1\n",
    "    loss_, _, loss_lf_, loss_hf_ = sess.run([loss, train_adam, loss_lf, loss_hf], feed_dict=train_dict)\n",
    "    LOSS = np.hstack((LOSS,loss_))\n",
    "    LOSS_LF = np.hstack((LOSS_LF,loss_lf_))\n",
    "    LOSS_HF = np.hstack((LOSS_HF,loss_hf_))\n",
    "    if n%1000 == 0:\n",
    "        print('n: %d, loss: %.3e, loss_lf: %.3e, loss_hf: %.3e'%(n, loss_, loss_lf_, loss_hf_))\n",
    "\n",
    "print('LBFG-S Optimizer')\n",
    "train_lbfgs.minimize(sess, feed_dict=train_dict, fetches=[loss, loss_lf, loss_hf], loss_callback=callback)\n",
    "\n",
    "print(\"Training is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS[20001] = LOSS[20000]\n",
    "LOSS_LF[20001] = LOSS_LF[20000]\n",
    "LOSS_HF[20001] = LOSS_HF[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager\n",
    "from matplotlib import rc\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)\n",
    "\n",
    "plt.figure(figsize=(10,10),dpi=300)\n",
    "\n",
    "plt.rc('font', size=20)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=20)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=22)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=20)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=20)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=22)    # legend fontsize\n",
    "plt.rc('figure', titlesize=22)  # fontsize of the figure title\n",
    "\n",
    "plt.plot(LOSS[0:62000] , 'bP-', markerfacecolor='none', linewidth=1.5, markersize = 5.0)\n",
    "plt.plot(LOSS_LF[0:62000] , 'rd-', markerfacecolor='none', linewidth=1.5, markersize = 5.0)\n",
    "plt.plot(LOSS_HF [0:62000], 'gs-' , markerfacecolor='none', linewidth=1.5, markersize = 5.0)\n",
    "\n",
    "plt.xlabel('Itteration', fontname = 'Times New Roman', fontsize=40)\n",
    "plt.ylabel('Error', fontname = 'Times New Roman', fontsize=40)\n",
    "plt.legend(['Total Loss','Loss of Low-Fidelity','Loss of High-Fidelity'],loc='upper right')\n",
    "#plt.title('Actual and Predicted Flow Curve for a Random Batch')\n",
    "\n",
    "plt.xticks(fontname = \"Times New Roman\", fontsize=30)\n",
    "plt.yticks(fontname = \"Times New Roman\", fontsize=30)\n",
    "#plt.ylim(0.6,13)\n",
    "#plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "#plt.savefig('Prediction_Batch09.png',bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0*42\n",
    "b=1*42\n",
    "\n",
    "ShR = np.power(10,XH2_test[a:b])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.rc('font', size=20)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=20)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=22)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=20)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=20)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=20)    # legend fontsize\n",
    "plt.rc('figure', titlesize=22)  # fontsize of the figure title\n",
    "\n",
    "plt.plot(ShR,np.power(10,y_hf_test[a:b]), 'bs--', linewidth=3.0)\n",
    "plt.plot(ShR,np.power(10,y_hf_ref[a:b]) , 'rd:', linewidth=3.0)\n",
    "#plt.plot(np.power(10,t_lf[401:442]),np.power(10,y_lf[401:442]) , 'rd:', linewidth=3.0)\n",
    "\n",
    "plt.xlabel('Shear Rate')\n",
    "plt.ylabel('Viscosity')\n",
    "plt.legend(['Predicted Data using MPINN','Actual Data from Experiment'])\n",
    "plt.title('Actual and Predicted Flow Curve for a Random Batch')\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.rc('font', size=20)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=20)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=22)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=20)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=20)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=20)    # legend fontsize\n",
    "plt.rc('figure', titlesize=22)  # fontsize of the figure title\n",
    "\n",
    "plt.plot(y_hf_ref,y_hf_test, 'kd', linewidth=5.0)\n",
    "plt.plot(np.linspace(-0.5, 1.5, 100),np.linspace(-0.5, 1.5, 100), 'r--', linewidth=3.0)\n",
    "plt.xlabel('Actual Data')\n",
    "plt.ylabel('Predicted Data')\n",
    "plt.legend(['Actual Regression','Perfect Regression'])\n",
    "plt.title('Regression Between Actual and Predicted Data')\n",
    "\n",
    "plt.grid(color='g', linestyle=':', linewidth=1)\n",
    "\n",
    "#plt.xlim(-0.5, 1.5)\n",
    "#plt.ylim(-0.5, 1.5)\n",
    "\n",
    "#plt.text(-4500, 11500, 'Pearson Regression Correlation is 0.94', style='italic',bbox={'facecolor': 'blue', 'alpha': 0.1, 'pad': 5})\n",
    "\n",
    "#plt.savefig('00 Research Codes/02_Multi_Fidelity/With_Randy/FX1_linear_regression.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
